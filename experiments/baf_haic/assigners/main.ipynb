{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import metrics\n",
    "\n",
    "from autodefer.models import haic\n",
    "from autodefer.utils import thresholding as t\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "root_path = '~'\n",
    "cfg_path = root_path + 'projects/learning-to-defer/experiments/baf_haic/assigners/cfg.yaml'\n",
    "with open(cfg_path, 'r') as infile:\n",
    "    cfg = yaml.safe_load(infile)\n",
    "\n",
    "RESULTS_PATH = cfg['results_path'] + cfg['exp_name'] + '/'\n",
    "MODELS_PATH = cfg['models_path'] + cfg['exp_name'] + '/'\n",
    "\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(MODELS_PATH, exist_ok=True)\n",
    "\n",
    "# import matplotlib; matplotlib.use('Agg')\n",
    "width = 450\n",
    "pd.set_option('display.width', width)\n",
    "np.set_printoptions(linewidth=width)\n",
    "pd.set_option('display.max_columns', 25)\n",
    "\n",
    "# DATA LOADING -------------------------------------------------------------------------------------\n",
    "with open(cfg['metadata'], 'r') as infile:\n",
    "    metadata = yaml.safe_load(infile)\n",
    "\n",
    "LABEL_COL = metadata['data_cols']['label']\n",
    "PROTECTED_COL = metadata['data_cols']['protected']\n",
    "CATEGORICAL_COLS = metadata['data_cols']['categorical']\n",
    "TIMESTAMP_COL = metadata['data_cols']['timestamp']\n",
    "\n",
    "SCORE_COL = metadata['data_cols']['score']\n",
    "BATCH_COL = metadata['data_cols']['batch']\n",
    "ASSIGNMENT_COL = metadata['data_cols']['assignment']\n",
    "DECISION_COL = metadata['data_cols']['decision']\n",
    "\n",
    "EXPERT_IDS = metadata['expert_ids']\n",
    "\n",
    "# train\n",
    "TRAIN_ENVS = {\n",
    "    tuple(exp_dir.split('#')): {\n",
    "        'train': pd.read_parquet(cfg['train_paths']['environments'] + exp_dir + '/train.parquet'),\n",
    "        'batches': pd.read_parquet(cfg['train_paths']['environments'] + exp_dir + '/batches.parquet'),\n",
    "        'capacity': pd.read_parquet(cfg['train_paths']['environments'] + exp_dir + '/capacity.parquet'),\n",
    "    }\n",
    "    for exp_dir in os.listdir(cfg['train_paths']['environments'])\n",
    "    if os.path.isdir(cfg['train_paths']['environments']+exp_dir)\n",
    "}\n",
    "\n",
    "# test\n",
    "test = pd.read_parquet(cfg['test_paths']['data'])\n",
    "test_experts_pred = pd.read_parquet(cfg['test_paths']['experts_pred'])\n",
    "TEST_ENVS = {\n",
    "    tuple(exp_dir.split('#')): {\n",
    "        'batches': pd.read_parquet(cfg['test_paths']['environments']+exp_dir+'/batches.parquet'),\n",
    "        'capacity': pd.read_parquet(cfg['test_paths']['environments']+exp_dir+'/capacity.parquet'),\n",
    "    }\n",
    "    for exp_dir in os.listdir(cfg['test_paths']['environments'])\n",
    "    if os.path.isdir(cfg['test_paths']['environments']+exp_dir)\n",
    "}\n",
    "\n",
    "# DEFINING FP COST ---------------------------------------------------------------------------------\n",
    "temp_train = TRAIN_ENVS[('large', 'regular')]['train'].copy()\n",
    "temp_train = temp_train[temp_train[TIMESTAMP_COL] != 6].drop(columns=TIMESTAMP_COL)\n",
    "ML_MODEL_THRESHOLD = t.calc_threshold_at_fpr(\n",
    "    y_true=temp_train[LABEL_COL],\n",
    "    y_score=temp_train[DECISION_COL],\n",
    "    fpr=cfg['fpr']\n",
    ")\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(\n",
    "    y_true=temp_train[LABEL_COL],\n",
    "    y_pred=(temp_train[DECISION_COL] >= ML_MODEL_THRESHOLD).astype(int),\n",
    "    labels=[0, 1]\n",
    ").ravel()\n",
    "print(f'FPR w/ full automation (train) = {fp/(fp+tn):.3f}')\n",
    "\n",
    "\"\"\" from derivatives\n",
    "fp_cost = t.calc_cost_at_threshold(\n",
    "    y_true=temp_train[LABEL_COL],\n",
    "    y_score=temp_train[DECISION_COL],\n",
    "    threshold=ml_model_threshold,\n",
    "    width=0.01\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# theoretical cost\n",
    "# t = fp_protected_penalty / (fp_protected_penalty + 1) <=> t.fp_protected_penalty + t = fp_protected_penalty <=> fp_protected_penalty(t-1) = -t <=> fp_protected_penalty= -t/t-1\n",
    "THEORETICAL_FP_COST = -ML_MODEL_THRESHOLD / (ML_MODEL_THRESHOLD - 1)\n",
    "\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(\n",
    "    y_true=temp_train[LABEL_COL],\n",
    "    y_pred=(temp_train[DECISION_COL] >= t.calc_threshold_with_cost(\n",
    "        y_true=temp_train[LABEL_COL],\n",
    "        y_score=temp_train[DECISION_COL],\n",
    "        fp_fn_cost_ratio=THEORETICAL_FP_COST)\n",
    "    ),\n",
    "    labels=[0, 1]\n",
    ").ravel()\n",
    "print(f'FPR at cost = {fp/(fp+tn):.3f}')\n",
    "# Risk Minimizing Assigners & Validation Set Construction ------------------------------------------\n",
    "VAL_ENVS = dict()\n",
    "VAL_X = None\n",
    "ATs = dict()\n",
    "RMAs = dict()\n",
    "for env_id in TRAIN_ENVS:\n",
    "    print(f'Loading {env_id} models')\n",
    "    batch_id, capacity_id = env_id\n",
    "    models_dir = f'{MODELS_PATH}{batch_id}_{capacity_id}/'\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    train_with_val = TRAIN_ENVS[env_id]['train']\n",
    "    train_with_val = train_with_val.copy().drop(columns=BATCH_COL)  # not needed\n",
    "    is_val = (train_with_val[TIMESTAMP_COL] == 6)\n",
    "    train_with_val = train_with_val.drop(columns=TIMESTAMP_COL)\n",
    "    train = train_with_val[~is_val].copy()\n",
    "    val = train_with_val[is_val].copy()\n",
    "\n",
    "    ATs[env_id] = haic.assigners.AlgorithmicTriage(\n",
    "        expert_ids=EXPERT_IDS,\n",
    "        outputs_dir=f'{models_dir}algorithmic_triage/'\n",
    "    )\n",
    "    ATs[env_id].fit(\n",
    "        train=train,\n",
    "        val=val,\n",
    "        categorical_cols=CATEGORICAL_COLS, score_col=SCORE_COL,\n",
    "        decision_col=DECISION_COL, ground_truth_col=LABEL_COL, assignment_col=ASSIGNMENT_COL,\n",
    "        hyperparam_space=cfg['human_expertise_model']['hyperparam_space'],\n",
    "        n_trials=cfg['human_expertise_model']['n_trials'],\n",
    "        random_seed=cfg['human_expertise_model']['random_seed'],\n",
    "    )\n",
    "\n",
    "    RMAs[env_id] = haic.assigners.RiskMinimizingAssigner(\n",
    "        expert_ids=EXPERT_IDS,\n",
    "        outputs_dir=f'{models_dir}human_expertise_model/',\n",
    "    )\n",
    "\n",
    "    RMAs[env_id].fit(\n",
    "        train=train,\n",
    "        val=val,\n",
    "        categorical_cols=CATEGORICAL_COLS, score_col=SCORE_COL,\n",
    "        decision_col=DECISION_COL, ground_truth_col=LABEL_COL, assignment_col=ASSIGNMENT_COL,\n",
    "        hyperparam_space=cfg['human_expertise_model']['hyperparam_space'],\n",
    "        n_trials=cfg['human_expertise_model']['n_trials'],\n",
    "        random_seed=cfg['human_expertise_model']['random_seed'],\n",
    "    )\n",
    "\n",
    "    VAL_ENVS[env_id] = dict()\n",
    "    if VAL_X is None:  # does not change w/ env\n",
    "        VAL_X_COMPLETE = val.copy()\n",
    "        VAL_X = VAL_X_COMPLETE.copy().drop(columns=[ASSIGNMENT_COL, DECISION_COL, LABEL_COL])\n",
    "    VAL_ENVS[env_id]['batches'] = (\n",
    "        TRAIN_ENVS[env_id]['batches']\n",
    "        .loc[val.index, ]\n",
    "        .copy()\n",
    "    )\n",
    "    VAL_ENVS[env_id]['capacity'] = (\n",
    "        TRAIN_ENVS[env_id]['capacity']\n",
    "        .loc[VAL_ENVS[env_id]['batches']['batch'].unique(), ]\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "# Evaluate Human Expertise Models ------------------------------------------------------------------\n",
    "def get_outcome(label, pred):\n",
    "    if pred == 1:\n",
    "        if label == 1:\n",
    "            o = 'tp'\n",
    "        elif label == 0:\n",
    "            o = 'fp'\n",
    "    elif pred == 0:\n",
    "        if label == 1:\n",
    "            o = 'fn'\n",
    "        elif label == 0:\n",
    "            o = 'tn'\n",
    "    return o\n",
    "\n",
    "OUTCOME_COL = 'error'\n",
    "expert_val_X = VAL_X_COMPLETE.copy()\n",
    "expert_val_X = expert_val_X[expert_val_X[ASSIGNMENT_COL] != EXPERT_IDS['model_ids'][0]]\n",
    "expert_val_X[OUTCOME_COL] = expert_val_X.apply(\n",
    "    lambda x: get_outcome(label=x[LABEL_COL], pred=x[DECISION_COL]),\n",
    "    axis=1,\n",
    ")\n",
    "expert_val_X = expert_val_X.drop(columns=[DECISION_COL, LABEL_COL])\n",
    "\n",
    "expert_model_results = dict()\n",
    "for model_name, model_set in {'rma': RMAs, 'at': ATs}.items():\n",
    "    for env_id in TRAIN_ENVS:\n",
    "        model = model_set[env_id]\n",
    "        X = (\n",
    "            expert_val_X.drop(columns=OUTCOME_COL) if model_name == 'rma'\n",
    "            else expert_val_X.drop(columns=[ASSIGNMENT_COL, OUTCOME_COL])\n",
    "        )\n",
    "        pred_proba = model.expert_model.predict_proba(X)\n",
    "\n",
    "        expert_model_results[tuple([model_name] + list(env_id))] = dict()\n",
    "        expert_model_results[tuple([model_name] + list(env_id))] = {\n",
    "            'training_set_size': (\n",
    "                (TRAIN_ENVS[env_id]['train'][ASSIGNMENT_COL] != EXPERT_IDS['model_ids'][0]).sum()\n",
    "            ),\n",
    "            'cross_entropy': metrics.log_loss(\n",
    "                y_true=expert_val_X[OUTCOME_COL], y_pred=pred_proba\n",
    "            ),\n",
    "            'avg_roc_auc': metrics.roc_auc_score(\n",
    "                y_true=expert_val_X[OUTCOME_COL], y_score=pred_proba,\n",
    "                multi_class='ovr',\n",
    "                average='macro'  # unweighted\n",
    "            ),\n",
    "            'fp_cross_entropy': metrics.log_loss(\n",
    "                y_true=(expert_val_X[OUTCOME_COL] == 'fp').astype(int),\n",
    "                y_pred=pred_proba[:, model.expert_model.classes_ == 'fp'].squeeze()\n",
    "            ),\n",
    "            'fp_roc_auc': metrics.roc_auc_score(\n",
    "                y_true=(expert_val_X[OUTCOME_COL] == 'fp').astype(int),\n",
    "                y_score=pred_proba[:, model.expert_model.classes_ == 'fp'].squeeze(),\n",
    "            ),\n",
    "            'fn_cross_entropy': metrics.log_loss(\n",
    "                y_true=(expert_val_X[OUTCOME_COL] == 'fn').astype(int),\n",
    "                y_pred=pred_proba[:, model.expert_model.classes_ == 'fn'].squeeze()\n",
    "            ),\n",
    "            'fn_roc_auc': metrics.roc_auc_score(\n",
    "                y_true=(expert_val_X[OUTCOME_COL] == 'fn').astype(int),\n",
    "                y_score=pred_proba[:, model.expert_model.classes_ == 'fn'].squeeze(),\n",
    "            ),\n",
    "        }\n",
    "expert_model_results = pd.DataFrame(expert_model_results).T.reset_index(drop=False)\n",
    "expert_model_results.columns = ['model_name', 'batch', 'capacity'] + list(expert_model_results.columns[3:])\n",
    "expert_model_results\n",
    "\n",
    "# EVALUATION FUNCTIONS -----------------------------------------------------------------------------\n",
    "def make_id_str(tpl):\n",
    "    printables = list()\n",
    "    for i in tpl:\n",
    "        if i == '':\n",
    "            continue\n",
    "        elif isinstance(i, (bool, int, float)):\n",
    "            printables.append(str(i))\n",
    "        else:\n",
    "            printables.append(i)\n",
    "\n",
    "    return '_'.join(printables)\n",
    "\n",
    "def evaluate(exp_id, exp_batches, exp_capacity, assignments, evaluator):\n",
    "    test_experts_pred_thresholded = test_experts_pred.copy()\n",
    "    test_experts_pred_thresholded[EXPERT_IDS['model_ids'][0]] = (\n",
    "            test_experts_pred_thresholded[EXPERT_IDS['model_ids'][0]] >= ML_MODEL_THRESHOLD\n",
    "    ).astype(int)\n",
    "    _decisions = haic.query_experts(\n",
    "        pred=test_experts_pred_thresholded,\n",
    "        assignments=assignments\n",
    "    )\n",
    "\n",
    "    evaluator.evaluate(\n",
    "        exp_id=exp_id,\n",
    "        assignments=assignments,\n",
    "        decisions=_decisions,\n",
    "        batches=exp_batches,\n",
    "        capacity=exp_capacity.T.to_dict(),\n",
    "        assert_capacity_constraints=False\n",
    "    )\n",
    "\n",
    "def product_dict(**kwargs):  # aux\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in itertools.product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "\n",
    "def make_params_combos(params_cfg):\n",
    "    params_list = list()\n",
    "    if not isinstance(params_cfg, list):\n",
    "        params_cfg = [params_cfg]\n",
    "\n",
    "    for cartesian_product_set in params_cfg:\n",
    "        for k, v in cartesian_product_set.items():\n",
    "            if isinstance(v, str):\n",
    "                cartesian_product_set[k] = [v]\n",
    "        for p in product_dict(**cartesian_product_set):\n",
    "            p_params = {**BASE_CFG, **p}\n",
    "            if p_params['fp_cost'] == 'theoretical':\n",
    "                p_params['fp_cost'] = THEORETICAL_FP_COST\n",
    "            if not (\n",
    "                p_params['calibration'] and  # useless to calibrate in these cases\n",
    "                (p_params['confidence_deferral'] or p_params['solver'] == 'random')\n",
    "            ):\n",
    "                params_list.append(p_params)\n",
    "\n",
    "    return params_list\n",
    "\n",
    "def make_assignments(X, envs, rma, exp_params):\n",
    "    env_id = (exp_params['batch'], exp_params['capacity'])\n",
    "    assigner_params = {k: v for k, v in exp_params.items() if k not in ['batch', 'capacity']}\n",
    "    params_to_record = {k: exp_params[k] for k in FIELDS}\n",
    "    exp_id = tuple([v for k, v in params_to_record.items()])\n",
    "    print(exp_id)\n",
    "    a = rma.assign(\n",
    "        X=X, score_col=SCORE_COL,\n",
    "        batches=envs[env_id]['batches'],\n",
    "        capacity=envs[env_id]['capacity'].T.to_dict(),\n",
    "        ml_model_threshold=ML_MODEL_THRESHOLD,\n",
    "        protected_col=(X[PROTECTED_COL] >= 50).map({True: 'Older', False: 'Younger'}),\n",
    "        protected_group='Older',\n",
    "        assignments_relative_path=make_id_str(exp_id),\n",
    "        **assigner_params\n",
    "    )\n",
    "\n",
    "    return exp_id, assigner_params, a\n",
    "\n",
    "def predicted_evaluation(X, assignments, rma, fp_cost):\n",
    "    X = X.copy().assign(**{ASSIGNMENT_COL: assignments})\n",
    "    X['index'] = X.index\n",
    "\n",
    "    pred_out_proba = rma.predict_outcome_probabilities(\n",
    "        X=X, score_col=SCORE_COL,\n",
    "        ml_model_threshold=ML_MODEL_THRESHOLD,\n",
    "        calibration=True\n",
    "    )\n",
    "    loss = fp_cost * pred_out_proba['fp'].sum() + pred_out_proba['fn'].sum()\n",
    "    tpr = pred_out_proba['tp'].sum() / (pred_out_proba['tp'].sum() + pred_out_proba['fn'].sum())\n",
    "    fpr = pred_out_proba['fp'].sum() / (pred_out_proba['tn'].sum() + pred_out_proba['fp'].sum())\n",
    "\n",
    "    protected_col = (X[PROTECTED_COL] >= 50).map({True: 'Older', False: 'Younger'})\n",
    "    is_protected_bool = (protected_col == 'Older')\n",
    "    fpr_disparity = (\n",
    "        (pred_out_proba[~is_protected_bool]['fp'].sum()\n",
    "           / (pred_out_proba[~is_protected_bool]['tn'].sum()\n",
    "              + pred_out_proba[~is_protected_bool]['fp'].sum()))\n",
    "        / (pred_out_proba[is_protected_bool]['fp'].sum()\n",
    "           / (pred_out_proba[is_protected_bool]['tn'].sum()\n",
    "              + pred_out_proba[is_protected_bool]['fp'].sum()))\n",
    "    )\n",
    "    return loss, tpr, fpr, fpr_disparity\n",
    "\n",
    "def make_assignments_and_predict_evaluate(X, envs, rma, exp_params):\n",
    "    exp_id, assigner_params, a = make_assignments(X=X, envs=envs, rma=rma, exp_params=exp_params)\n",
    "    pred_loss, pred_tpr, pred_fpr, pred_fpr_disparity = predicted_evaluation(\n",
    "        X=X, assignments=a, rma=rma, fp_cost=assigner_params['fp_cost'],\n",
    "    )\n",
    "    return exp_id, pred_loss, pred_tpr, pred_fpr, pred_fpr_disparity\n",
    "\n",
    "\n",
    "ENV_FIELDS = ['batch', 'capacity']\n",
    "ASSIGNER_FIELDS = [\n",
    "    'confidence_deferral', 'solver', 'calibration', 'fp_cost', 'fp_protected_penalty',\n",
    "    'dynamic', 'target_fpr_disparity', 'fpr_learning_rate', 'fpr_disparity_learning_rate'\n",
    "]\n",
    "FIELDS = ENV_FIELDS + ASSIGNER_FIELDS\n",
    "print(tuple(FIELDS))\n",
    "\n",
    "BASE_CFG = cfg['base_cfg']\n",
    "\"\"\"\n",
    "# EXPERIMENTS --------------------------------------------------------------------------------------\n",
    "val_results_dict = dict()\n",
    "if cfg['n_jobs'] > 1:\n",
    "    Parallel(n_jobs=cfg['n_jobs'])(\n",
    "        delayed(make_assignments)(\n",
    "            X=VAL_X,\n",
    "            envs=VAL_ENVS,\n",
    "            rma=RMAs[(exp_params['batch'], exp_params['capacity'])],\n",
    "            exp_params=exp_params\n",
    "        )\n",
    "        for exp_params in make_params_combos(cfg['experiments'])\n",
    "    )\n",
    "\n",
    "for exp_params in make_params_combos(cfg['experiments']):\n",
    "    exp_id, pred_loss, pred_tpr, pred_fpr, pred_fpr_disparity = (\n",
    "        make_assignments_and_predict_evaluate(\n",
    "            X=VAL_X,\n",
    "            envs=VAL_ENVS,\n",
    "            rma=RMAs[(exp_params['batch'], exp_params['capacity'])],\n",
    "            exp_params=exp_params)\n",
    "    )\n",
    "    val_results_dict[exp_id] = dict(\n",
    "        pred_loss=pred_loss, pred_tpr=pred_tpr, pred_fpr=pred_fpr,\n",
    "        pred_fpr_disparity=pred_fpr_disparity\n",
    "    )\n",
    "\n",
    "val_results = pd.DataFrame(val_results_dict).T.reset_index(drop=False)\n",
    "val_results.columns = FIELDS + ['pred_loss', 'pred_tpr', 'pred_fpr', 'pred_fpr_disparity']\n",
    "val_results\n",
    "\n",
    "# %%\n",
    "val_results = val_results.drop(\n",
    "    columns=['dynamic', 'target_fpr_disparity', 'fpr_learning_rate', 'fpr_disparity_learning_rate']\n",
    ")\n",
    "# RENAME FOR PLOTS\n",
    "col_renamings = {\n",
    "    'batch': 'Batch',\n",
    "    'capacity': 'Capacity',\n",
    "    'confidence_deferral': 'Confidence Deferral',\n",
    "    'calibration': 'Calibration',\n",
    "    'solver': 'Solver',\n",
    "    'fp_cost': 'lambda',\n",
    "    'fp_protected_penalty': 'alpha',\n",
    "    'pred_loss': 'Loss',\n",
    "    'pred_fpr': 'Predicted FPR',\n",
    "    'pred_tpr': 'Predicted TPR',\n",
    "    'pred_fpr_disparity': 'Predicted FPR Parity'\n",
    "}\n",
    "\n",
    "# %%\n",
    "architecture_results = val_results[\n",
    "    (val_results['fp_cost'] == THEORETICAL_FP_COST) &\n",
    "    (val_results['fp_protected_penalty'] == 0)\n",
    "]\n",
    "(\n",
    "    architecture_results\n",
    "    .groupby(['confidence_deferral', 'solver', 'calibration'])\n",
    "    .mean()\n",
    "    .sort_values(by='pred_loss')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# %%\n",
    "sns.scatterplot(\n",
    "    data=architecture_results, x='solver', y='pred_loss',\n",
    "    hue='calibration', style='confidence_deferral',\n",
    "    alpha=0\n",
    ")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "conf_def_false = architecture_results[architecture_results['confidence_deferral'] == False]\n",
    "m = sns.stripplot(\n",
    "    data=conf_def_false, x='solver', y='pred_loss', hue='calibration',\n",
    "    marker='o', edgecolor='grey', jitter=1,\n",
    ")\n",
    "\n",
    "conf_def_true = architecture_results[architecture_results['confidence_deferral'] == True]\n",
    "n = sns.stripplot(\n",
    "    data=conf_def_true, x='solver', y='pred_loss', hue='calibration',\n",
    "    marker='X', edgecolor='grey', jitter=1,\n",
    ")\n",
    "plt.legend(handles, labels)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "fp_cost_results = val_results[\n",
    "    (val_results['confidence_deferral'] == False) &\n",
    "    (val_results['solver'] == 'scheduler') &\n",
    "    (val_results['calibration'] == True) &\n",
    "    (val_results['fp_protected_penalty'] == 0)\n",
    "]\n",
    "\n",
    "(\n",
    "    fp_cost_results\n",
    "    .pivot(index='fp_cost', columns=['batch', 'capacity'], values='pred_fpr')\n",
    "    .T.reset_index()\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=fp_cost_results[fp_cost_results['fp_cost'].isin([THEORETICAL_FP_COST, 0.05, 1, 2])],\n",
    "    x='fp_cost', y='pred_fpr', markers=True,\n",
    "    hue='capacity', style='batch',\n",
    "    palette='colorblind'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "sns.lineplot(\n",
    "    data=fp_cost_results[fp_cost_results['fp_cost'] < 1],\n",
    "    x='fp_cost', y='pred_fpr', markers=True,\n",
    "    hue='capacity', style='batch',\n",
    "    palette='colorblind'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "fairness_results = val_results[\n",
    "    (val_results['confidence_deferral'] == False)\n",
    "    & (val_results['solver'] == 'scheduler')\n",
    "    & (val_results['calibration'] == True)\n",
    "]\n",
    "sns.scatterplot(\n",
    "    data=fairness_results,\n",
    "    x='pred_fpr',\n",
    "    y='pred_tpr',\n",
    "    hue='fp_protected_penalty'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# fairness_results['violation'] = (fairness_results['pred_fpr'] - cfg['fpr']).abs()\n",
    "fairness_results_below_fpr = fairness_results[fairness_results['pred_fpr'] <= cfg['fpr']]\n",
    "fairness_results_below_fpr[\n",
    "    (fairness_results_below_fpr['batch'] == 'large')\n",
    "    & (fairness_results_below_fpr['capacity'] == 'inconstant')\n",
    "].sort_values(by=['fp_protected_penalty', 'pred_fpr'])\n",
    "fairness_results_at_fpr = (\n",
    "    fairness_results_below_fpr\n",
    "    # .sort_values(by='violation', ascending=True)\n",
    "    .sort_values(by='pred_fpr', ascending=False)\n",
    "    .groupby(['batch', 'capacity', 'fp_protected_penalty'])\n",
    "    .head(1)\n",
    "    .sort_values(by=['batch', 'capacity', 'fp_protected_penalty'])\n",
    ")\n",
    "sns.lineplot(\n",
    "    data=fairness_results_at_fpr,\n",
    "    x='pred_tpr', y='pred_fpr_disparity', markers=True,\n",
    "    hue='capacity', style='batch',\n",
    "    palette='colorblind',\n",
    "    sort=False,\n",
    ")\n",
    "plt.xlim(0.5, 0.7)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# TEST SET EVALUATION ------------------------------------------------------------------------------\n",
    "TEST_X = test.drop(columns=[TIMESTAMP_COL, LABEL_COL])\n",
    "test_experts_pred_thresholded = test_experts_pred.copy()\n",
    "test_experts_pred_thresholded[EXPERT_IDS['model_ids'][0]] = (\n",
    "        test_experts_pred_thresholded[EXPERT_IDS['model_ids'][0]] >= ML_MODEL_THRESHOLD\n",
    ").astype(int)\n",
    "test_eval = haic.HAICEvaluator(\n",
    "    y_true=test[LABEL_COL],\n",
    "    experts_pred=test_experts_pred,\n",
    "    exp_id_cols=FIELDS\n",
    ")\n",
    "\n",
    "# Baselines\n",
    "for env_id in TEST_ENVS:\n",
    "    a = ATs[env_id].assign(\n",
    "        X=TEST_X, score_col=SCORE_COL, ml_model_threshold=ML_MODEL_THRESHOLD,\n",
    "        fp_cost=THEORETICAL_FP_COST,\n",
    "        batches=TEST_ENVS[env_id]['batches'],\n",
    "        capacity=TEST_ENVS[env_id]['capacity'].T.to_dict(),\n",
    "        assignments_relative_path=make_id_str(env_id),\n",
    "    )\n",
    "    d = haic.query_experts(\n",
    "        pred=test_experts_pred_thresholded,\n",
    "        assignments=a\n",
    "    )\n",
    "    test_eval.evaluate(\n",
    "        exp_id=[env_id[0], env_id[1], False, 'algorithmic_triage', True, THEORETICAL_FP_COST, 0, False, 1.2, 0, 0],\n",
    "        assignments=a,\n",
    "        decisions=d,\n",
    "        assert_capacity_constraints=False\n",
    "    )\n",
    "\n",
    "# L2A\n",
    "for env_id, rma in RMAs.items():\n",
    "    if 'test' not in rma.outputs_dir:  # avoid double change\n",
    "        test_path = rma.outputs_dir[:-1] + '_test/'\n",
    "        os.makedirs(test_path, exist_ok=True)\n",
    "        rma.outputs_dir = test_path\n",
    "\n",
    "to_test = list()\n",
    "\n",
    "cost_sensitive_cfgs = val_results[\n",
    "    (val_results['confidence_deferral'] == False) &\n",
    "    (val_results['solver'] == 'scheduler') &\n",
    "    (val_results['calibration'] == True) &\n",
    "    (val_results['fp_cost'] == THEORETICAL_FP_COST) &\n",
    "    (val_results['fp_protected_penalty'] == 0)\n",
    "]\n",
    "for ix, row in cost_sensitive_cfgs.iterrows():\n",
    "    exp_params = {k: v for k, v in dict(row).items() if k in FIELDS}\n",
    "    to_test.append({**BASE_CFG, **exp_params})\n",
    "\n",
    "for ix, row in fairness_results_at_fpr.iterrows():\n",
    "    exp_params = {k: v for k, v in dict(row).items() if k in FIELDS}\n",
    "    to_test.append({**BASE_CFG, **exp_params})\n",
    "\n",
    "if cfg['n_jobs'] > 1:\n",
    "    Parallel(n_jobs=cfg['n_jobs'])(\n",
    "        delayed(make_assignments)(\n",
    "            X=TEST_X,\n",
    "            envs=TEST_ENVS,\n",
    "            rma=RMAs[(exp_params['batch'], exp_params['capacity'])],\n",
    "            exp_params=exp_params\n",
    "        )\n",
    "        for exp_params in to_test\n",
    "    )\n",
    "else:\n",
    "    for exp_params in to_test:\n",
    "        exp_id, assigner_params, a = make_assignments(\n",
    "            X=TEST_X,\n",
    "            envs=TEST_ENVS,\n",
    "            rma=RMAs[(exp_params['batch'], exp_params['capacity'])],\n",
    "            exp_params=exp_params\n",
    "        )\n",
    "        d = haic.query_experts(\n",
    "            pred=test_experts_pred_thresholded,\n",
    "            assignments=a\n",
    "        )\n",
    "        test_eval.evaluate(\n",
    "            exp_id=exp_id,\n",
    "            assignments=a,\n",
    "            decisions=d,\n",
    "            assert_capacity_constraints=False\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
